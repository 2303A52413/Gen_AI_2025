{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpHWGD91X9P4js681GmYM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52413/Gen_AI_2025/blob/main/2303A52413_Gen_AI_W3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (1 ponto) Write Python code without using any libraries to find the value of x at which the\n",
        "function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f(x) = 5x\n",
        "4 + 3x\n",
        "2 + 10"
      ],
      "metadata": {
        "id": "W3zJdhhie87J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f(x):\n",
        "    return 5 * x*4 + 3 * x*2 + 10\n",
        "\n",
        "def df_dx(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent():\n",
        "\n",
        "    x = 0.0\n",
        "    learning_rate = 0.01\n",
        "    max_iters = 1000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        gradient = df_dx(x)\n",
        "        new_x = x - learning_rate * gradient\n",
        "\n",
        "\n",
        "        if abs(new_x - x) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "        x = new_x\n",
        "\n",
        "    return x\n",
        "\n",
        "x_min = gradient_descent()\n",
        "print(f\"The minimum value of f(x) is at x = {x_min}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFMyB5IxfBLW",
        "outputId": "63194bf1-10cf-45d8-fd61-3d8bcd084aa7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 1 iterations.\n",
            "The minimum value of f(x) is at x = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 ponto) Write Python code without using any libraries to find the value of x and y at which the\n",
        "function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f(x) = 3x\n",
        "2 + 5e\n",
        "−y + 10"
      ],
      "metadata": {
        "id": "HCadsEaieopb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * (2.71828 ** (-y)) + 10\n",
        "\n",
        "def gradient_descent():\n",
        "\n",
        "    x = 0.0\n",
        "    y = 0.0\n",
        "    learning_rate = 0.01\n",
        "    max_iters = 1000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        dg_dx = 6 * x\n",
        "        dg_dy = -5 * (2.71828 ** (-y))\n",
        "\n",
        "\n",
        "        new_x = x - learning_rate * dg_dx\n",
        "        new_y = y - learning_rate * dg_dy\n",
        "\n",
        "        if abs(new_x - x) < tolerance and abs(new_y - y) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "        x = new_x\n",
        "        y = new_y\n",
        "\n",
        "    return x, y\n",
        "\n",
        "x_min, y_min = gradient_descent()\n",
        "print(f\"The minimum value of g(x, y) is at x = {x_min}, y = {y_min}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS5lnw9RerZD",
        "outputId": "dfd5d023-b29a-4581-84ad-7c1889898a0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of g(x, y) is at x = 0.0, y = 3.933762227862872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". (1 ponto) Write Python code without using any libraries to find the value of x at which the\n",
        "sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent\n",
        "Algorithm.\n",
        "z(x) = 1\n",
        "1 + e\n",
        "−x"
      ],
      "metadata": {
        "id": "WdoKj781fJ6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def z(x):\n",
        "    return 1 / (1 + 2.71828**(-x))\n",
        "\n",
        "def dz_dx(x):\n",
        "\n",
        "    sigmoid = z(x)\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n",
        "def gradient_descent():\n",
        "\n",
        "    x = 0.0\n",
        "    learning_rate = 0.01\n",
        "    max_iters = 1000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        gradient = dz_dx(x)\n",
        "\n",
        "        new_x = x - learning_rate * gradient\n",
        "\n",
        "        if abs(new_x - x) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            break\n",
        "        x = new_x\n",
        "\n",
        "    return x\n",
        "\n",
        "x_min = gradient_descent()\n",
        "print(f\"The minimum value of z(x) is at x = {x_min}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU6-p_PmfJkB",
        "outputId": "e93087bd-99f2-405d-9424-cceb57062196"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The minimum value of z(x) is at x = -1.8618359316575634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1 ponto) Write Python code without using any libraries to find the value of optimal values of\n",
        "model parameters M and C such that the model’s Square Error Value shown in equation 4 will\n",
        "be minimum. It means model gives output close to expected output as shown in\n",
        "SE = (ExpectedOutput − P redictedOutput)\n",
        "2\n"
      ],
      "metadata": {
        "id": "XxrpMCTXffNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def square_error(M, C, expected, x):\n",
        "    predicted = M * x + C\n",
        "    return (expected - predicted)**2\n",
        "\n",
        "def gradients(M, C, expected, x):\n",
        "    predicted = M * x + C\n",
        "    error = expected - predicted\n",
        "    dM = -2 * x * error\n",
        "    dC = -2 * error\n",
        "    return dM, dC\n",
        "\n",
        "def gradient_descent():\n",
        "\n",
        "    M, C = 0.0, 0.0\n",
        "    learning_rate = 0.01\n",
        "    max_iters = 1000\n",
        "    tolerance = 1e-6\n",
        "\n",
        "    data = [(1, 2), (2, 4), (3, 6), (4, 8)]\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        total_dM, total_dC = 0.0, 0.0\n",
        "\n",
        "        for x, expected in data:\n",
        "            dM, dC = gradients(M, C, expected, x)\n",
        "            total_dM += dM\n",
        "            total_dC += dC\n",
        "\n",
        "        new_M = M - learning_rate * total_dM / len(data)\n",
        "        new_C = C - learning_rate * total_dC / len(data)\n",
        "\n",
        "        if abs(new_M - M) < tolerance and abs(new_C - C) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "        M, C = new_M, new_C\n",
        "\n",
        "    return M, C\n",
        "\n",
        "optimal_M, optimal_C = gradient_descent()\n",
        "print(f\"The optimal values are M = {optimal_M}, C = {optimal_C}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBvmiANUflWW",
        "outputId": "198383b3-4a68-4cc0-949b-680f0c71a040"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values are M = 1.9896587550255742, C = 0.030404521305361965\n"
          ]
        }
      ]
    }
  ]
}